{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\0-PROJETOS\\2-GIT\\Deepsort_Yolov5\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# Get main path\n",
    "MAIN_PATH = os.getcwd()\n",
    "\n",
    "sys.path.append(f'{MAIN_PATH}\\\\DeepSORT_YOLOv5_Pytorch\\\\')\n",
    "sys.path.append(f'{MAIN_PATH}\\\\DeepSORT_YOLOv5_Pytorch\\\\deep_sort\\\\deep\\\\checkpoint\\\\')\n",
    "\n",
    "from yolov5.utils.general import non_max_suppression, scale_coords, xyxy2xywh\n",
    "from yolov5.utils.torch_utils import select_device\n",
    "from yolov5.utils.datasets import letterbox\n",
    "\n",
    "sys.path.append(f'{MAIN_PATH}\\\\DeepSORT_YOLOv5_Pytorch\\\\yolov5\\\\models\\\\')\n",
    "from experimental import attempt_load\n",
    "\n",
    "from utils_ds.parser import get_config\n",
    "from deep_sort import build_tracker\n",
    "\n",
    "from utils_ds.parser import get_config\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import mediapipe as mp\n",
    "cudnn.benchmark = True\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config File Deepsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_config_deepsort(save_path):\n",
    "    path_yaml = MAIN_PATH + '\\\\DeepSORT_YOLOv5_Pytorch\\\\deep_sort\\\\deep\\\\checkpoint\\\\ckpt.t7'\n",
    "\n",
    "    config = {\n",
    "        'DEEPSORT': {\n",
    "            'REID_CKPT': path_yaml,\n",
    "            'MAX_DIST': 0.2,\n",
    "            'MIN_CONFIDENCE': 0.3,\n",
    "            'NMS_MAX_OVERLAP': 0.5,\n",
    "            'MAX_IOU_DISTANCE': 0.7,\n",
    "            'MAX_AGE': 70,\n",
    "            'N_INIT': 3,\n",
    "            'NN_BUDGET': 100\n",
    "        }\n",
    "    }\n",
    "    with open(save_path, 'w') as file:\n",
    "        yaml.dump(config, file)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aply Tracking on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_track(im0, \n",
    "                detector,\n",
    "                deepsort,\n",
    "                img_size,\n",
    "                device,\n",
    "                conf_thres,\n",
    "                iou_thres,\n",
    "                classes):\n",
    "    \"\"\"\n",
    "        Deep Sort Tracking for YOLOv5 Inference method.\n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocess ************************************************************\n",
    "    # Padded resize\n",
    "    img = letterbox(im0, new_shape=img_size)[0]\n",
    "    \n",
    "    # Convert:\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    # numpy to tensor\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    s = '%gx%g ' % img.shape[2:]  # print string\n",
    "\n",
    "    # Detection time *********************************************************\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        pred = detector(img)[0]  # list: bz * [ (#obj, 6)]\n",
    "\n",
    "    # Apply NMS and filter object other than person (cls:0)\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres,\n",
    "                               classes=classes)\n",
    "\n",
    "    # get all obj ************************************************************\n",
    "    det = pred[0]  # for video, bz is 1\n",
    "    if det is not None and len(det):  # det: (#obj, 6)  x1 y1 x2 y2 conf cls\n",
    "\n",
    "        # Rescale boxes from img_size to original im0 size\n",
    "        det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "        # Print results. statistics of number of each obj\n",
    "        for c in det[:, -1].unique():\n",
    "            n = (det[:, -1] == c).sum()  # detections per class\n",
    "            s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
    "            \n",
    "        bbox_xywh = xyxy2xywh(det[:, :4]).cpu()\n",
    "        confs = det[:, 4:5].cpu()\n",
    "        class_det = det[:, -1].cpu()\n",
    "        # ****************************** deepsort ****************************\n",
    "        outputs = deepsort.update(bbox_xywh, confs, im0, class_det)\n",
    "        # (#ID, 5) x1,y1,x2,y2,track_ID\n",
    "    else:\n",
    "        outputs = torch.zeros((0, 5))\n",
    "        confs = torch.zeros((0, 1))\n",
    "                            \n",
    "    return outputs, confs\n",
    "\n",
    "###########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_color_for_labels(label):\n",
    "        \"\"\"\n",
    "            Simple function that adds fixed color depending on the class.\n",
    "        \"\"\"\n",
    "\n",
    "        color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n",
    "        return tuple(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(image,  outputs, confs, class_names):\n",
    "    \"\"\"\n",
    "        Draw bounding box and class name with confidence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the outputs\n",
    "    bbox_xyxy  = outputs[:, :4]\n",
    "    identities = outputs[:, -2]\n",
    "    class_det  = outputs[:, -1]\n",
    "\n",
    "    for i, (bbox, conf, classe) in enumerate(zip(bbox_xyxy, confs, class_det)):\n",
    "        # Get the coordinates\n",
    "        x1, y1, x2, y2 = bbox.astype(int)\n",
    "        confidence = str(conf.numpy()[0])[2:4]\n",
    "        name = class_names[int(classe)]\n",
    "\n",
    "        id = int(identities[i]) if identities is not None else 0 \n",
    "        color  = compute_color_for_labels(id)\n",
    "\n",
    "        # text = f'id:{id}  class:{names}  conf: {confidence}%'\n",
    "        text = f'id:{id} conf : {confidence}% class: {name}'\n",
    "        t_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_PLAIN, 2 , 2)[0]\n",
    "        \n",
    "        # Draw Detections in the image\n",
    "        cv2.rectangle(image, (x1, y1), (x2,y2), color, 3)\n",
    "        cv2.rectangle(image, (x1, y1), (x1+t_size[0]+3,y1+t_size[1]+4), color,-1)\n",
    "        cv2.putText(image, text, (x1,y1+t_size[1]+4), cv2.FONT_HERSHEY_PLAIN, 1, [255,255,255], 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_path):\n",
    "    \"\"\"\n",
    "        Main function to run Deep Sort Tracking with YOLOv5 Inference.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defined parameters to img size:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    success, frame = cap.read()\n",
    "    img_size       = (max(frame.shape[:2]), (max(frame.shape[:2])))\n",
    "    img_size       = (640, 480)\n",
    "\n",
    "    # Set which frequency detection models will process frames:\n",
    "    frame_interval = 2\n",
    "\n",
    "    # Auxiliars:\n",
    "    idx_frame = 0\n",
    "    last_out  = None\n",
    "\n",
    "    # Tracking\n",
    "    while cap.isOpened():\n",
    "\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        # Check if video ends:\n",
    "        if not success:\n",
    "            \n",
    "            # Repeat video:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            success, frame = cap.read()\n",
    "\n",
    "        # Make a frame copy and get black board auxiliar:\n",
    "        frame_copy  = frame.copy()\n",
    "\n",
    "        #####################################################################################################\n",
    "        # Apply Deep Sort Track:\n",
    "        if idx_frame % frame_interval == 0:\n",
    "            outputs, confs = image_track(frame,\n",
    "                                        detector,\n",
    "                                        deepsort,\n",
    "                                        img_size,\n",
    "                                        device,\n",
    "                                        conf_thres,\n",
    "                                        iou_thres,\n",
    "                                        classes)\n",
    "            \n",
    "            # Update last output detection:\n",
    "            last_out = outputs\n",
    "        \n",
    "        # Use prediction of the previous frames:\n",
    "        else: outputs = last_out  \n",
    "            \n",
    "        #####################################################################################################\n",
    "\n",
    "        # For each YOLOv5 Detection after Deep Sort, draw bbox and apply pose estimation:\n",
    "        if len(outputs) > 0:\n",
    "            frame = draw_bounding_box(frame, outputs, confs, names)  # BGR\n",
    "            \n",
    "            # Use Mediapipe Pose detection applying to YOLOv5 detected objects:\n",
    "            if idx_frame % 5*frame_interval == 0:\n",
    "                pose_output   = np.zeros_like(frame)\n",
    "                    \n",
    "        ##############################################################################################\n",
    "            \n",
    "        # Keyboard Controls:\n",
    "        key = cv2.waitKey(1) or 0xff   \n",
    "        if key == ord('k') or key == ord('q'): break\n",
    "            \n",
    "        # Show output:\n",
    "        cv2.imshow('Detection:', frame)\n",
    "        \n",
    "        # Update frame counter:\n",
    "        idx_frame += 1\n",
    "        if idx_frame == 3*frame_interval: idx_frame = 0\n",
    "        \n",
    "        # Clear output:\n",
    "        clear_output(wait=False)\n",
    "        \n",
    "    #######################################################################################################\n",
    "    \n",
    "    # Release Video:\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv5 Detector:\n",
    "device = select_device('0')\n",
    "model_weights = torch.load(f'{MAIN_PATH}\\\\DeepSORT_YOLOv5_Pytorch\\\\yolov5\\\\weights\\\\yolov5s.pt', map_location=device)\n",
    "model_weights['model'] = model_weights['model'].float()\n",
    "detector = model_weights['model'].to(device).eval()\n",
    "names = detector.module.names if hasattr(detector, 'module') else detector.names\n",
    "\n",
    "# Config Deepsort:\n",
    "save_path = MAIN_PATH + '\\\\DeepSORT_YOLOv5_Pytorch\\\\configs\\\\deep_sort.yaml'\n",
    "deepsort_config = create_file_config_deepsort(save_path)\n",
    "\n",
    "# Configurations for car detection:\n",
    "conf_thres = 0.65\n",
    "iou_thres  = 0.10\n",
    "classes    = [2] # classe for car detection\n",
    "\n",
    "palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1) # Pallete for colors of bounding boxes\n",
    "video_path = 'media/cars.mp4'\n",
    "\n",
    "# Load Deep Sort:\n",
    "cfg = get_config()\n",
    "cfg.merge_from_file(deepsort_config)\n",
    "deepsort = build_tracker(cfg, use_cuda=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
